---
title: "Practical Machine Learning Course - Final Prediction Assignment"
university: "John Hopkins University - Provided via Coursera platform"
author: "Priyam Banerjee"
date: "February 27, 2016"
output: html_document
---

#__GitHub Repo for this Project:__ [__Practical ML Course Assignment GitHUb Repo__](https://github.com/priyamb/PracticalML_Coursera_JH_Feb2016/tree/master/FINAL_PRED_ASSIGNMENT)

###__Introduction__
This markdown document has been created to document all the steps followed to complete and submit the final project of the __Practical Machine Learning__ course offered by [__John Hopkins University__](https://www.jhu.edu/) via the [Coursera](https://www.coursera.org/) online platform. This document highlights the steps I followed in analysing the data, preprocessing and finally testing a couple of prediction techniques taught in the class to find out the most effective prediction solution.

The project description as provided in the assignment along with sources of data to be used is listed in the following 2 sections. Following them, we shall get into the sections to obtain data, clean-up or preprocessing and model building and testing.

###__Background__

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. 

In this project, our goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways.

###__Data__

The training data for this project are available here: [Training Data](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv)

The test data are available here: [Testing Data](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv)

The data for this project come from this source: [Groupware@LES - Human Activity Recognition](http://groupware.les.inf.puc-rio.br/har).

###__Load Libraries__

This step will call in the packages from which we shall use multiple fucntions to do the assignment:

```{r loadLibs}
library(caret) #We'll mostly use the data partitioning and train functions from Caret
library(rpart) #For decision trees
library(rattle) #For fancy decision tree plots
library(randomForest) #For random forests
```

###__Loading Raw Data__

Let us now write a simple dataload step to load the data. The codes in the lines below will load the data directly from the URL provided: 
```{r loadData}
trnLnk <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
tstLnk <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

training_data <- read.csv(url(trnLnk),na.strings=c("NA","#DIV/0!",""),header=T)
testing_data <- read.csv(url(tstLnk),na.strings=c("NA","#DIV/0!",""),header=T)

#Check dimensions of train dataset
dim(training_data)

#Check dimensions of test dataset
dim(testing_data)

```
The training dataset which will be used for fitting our model contains 19,622 observations and 160 variables while the testing data which will be used to validate our model contains 20 observations and 160 variables. The "classe" variable is the response variable which our model will try to predict. With the data loaded into the memory, we'll move into an anlysis of the raw data.

###__Analysis of Raw Data__

Let us also take a look at the distribution of the target variable "Classe" in _Figure 1_ below from the raw training data:
```{r plot_target_rawTRN, echo=FALSE}
frqt=paste(round(table(training_data$classe)/dim(training_data)[1]*100,2),"%",sep='')
yfrqt=table(training_data$classe)
ylm = ceiling(max(yfrqt)/1000)*1000
plotTgt = plot(training_data$classe,main="Figure 1: Plot of Classe in Training dataset",xlab="Classe",ylab="Count",ylim=c(0,ylm))
text(x=plotTgt,y=yfrqt,label=frqt,pos=3,cex=0.9,col="blue")
```

From the above _Figure 1_, the target seems to be a clean variable. Class "A" forms the bulk of numbers while the rest of the classes are almost similar in distribution. With that, let us focus our attention to the predictors.

First, we check the columns and rows to see if there are a sizeable number of complete cases:
```{r compltcase}
sum(complete.cases(training_data))
```
The above shows that there are no cases which is complete. That is, each observation has either some NA or incorrect values for certain variables that cannot be used by the prediction algorithms. Hence, we'll now follow data clean-up steps.

###__Data Clean Up__
First, drop the columns which include serial numbers, time-stamps and names:
```{r dropUnwtdCols}
##Create list of unwanted fields:
trnRemCols <- grepl("^X|timestamp|window|user_name",names(training_data))
tstRemCols <- grepl("^X|timestamp|window|user_name",names(testing_data))

#Remove unwanted fields
trnRmUnwtdCols <- training_data[,!trnRemCols]
tstRmUnwtdCols <- testing_data[,!tstRemCols]
```
Next, find and drop the fields which have "near-zero variance". These won't have a great effect on the prediction algorithm. In the process, we'll also drop columns with NAs and create the cleaned-up training and test datasets:
```{r dropNZVcols}
#Create list of near zero variance fields
NearZeroVar <- nearZeroVar(trnRmUnwtdCols,saveMetrics=T)

#Remove near zero variance fields
trnRmZVCols <- trnRmUnwtdCols[,!NearZeroVar$nzv]
tstRmZVCols <- tstRmUnwtdCols[,!NearZeroVar$nzv]

#Remove fields with NAs
trnNArmCondn <- (colSums(is.na(trnRmZVCols))!=0)
tstNArmCondn <- (colSums(is.na(tstRmZVCols))!=0)

trnRmNACols <- trnRmZVCols[,!trnNArmCondn]
tstRmNACols <- tstRmZVCols[,!tstNArmCondn]

#New Training and Testing Datasets after clean-up
trnDataNew <- trnRmNACols
tstDataNew <- tstRmNACols

dim(trnDataNew); dim(tstDataNew)
```
Post the dataset clean-up, we are left with 53 variables and 19,622 observations in the cleaned up train data and 53 variables and 20 observations in the testing data. With a clean data, we shall now move into sample division step.

###__Prediction Data Design__

First set a seed to enable reproducing results for multiple runs. Then, we'll create a division in the cleaned-up training dataset (_trnDataNew_) to divide it into model training (70%) and model testing (30%) subsets. The cleaned up testing dataset (_tstDataNew_) will be used for model validation.
```{r setSeed}
set.seed(19284601) #For reproducing results

#Train-model Validation Partition
intrain <- createDataPartition(y=trnDataNew$classe,p=0.7,list=F)

modTRNSample <- trnDataNew[intrain,] #To be used for model training
modTSTSample <- trnDataNew[-intrain,] #To be used for testing accuracy of models

dim(modTRNSample); dim(modTSTSample)
```
Post the division, the model training data contains 70% (of 19,622) or 13,737 observations and 53 variables and the model testing data contains 30% (of 19,622) or 5,885 observations and 53 variables. 

Now, let us get into the model building exercise. We shall first try a decision tree analysis.

###__Model Building and Testing__
We shall now build a predictive model for activity recognition using the __Decision Tree__ approach. We first use the tree approach as it is easiest to interpret and in a non-linear set-up can do better prediction than regression models. As we know, we need to have a good trade-off between accuracy and interpretability. 

So, we'll see how best a predictive algorithm this approach is able to build in the next few lines.

#####__Method 1: Decision Tree__
```{r dtree}
#Fit a decision tree model
modDTfit <- train(classe~.,method="rpart",data=modTRNSample)
```

_Figure 2_ shows the tree that has been grown out of the step above:
```{r dtreePlot, echo=FALSE}
rattle::fancyRpartPlot(modDTfit$finalModel)
```


As we see above in _Figure 2_, this approach creates a very easily interpretable classification mechanism. _For example_, in the above, *roll_belt < 130* and *pitch_forearm < -26* combination captures 9% of observations and 96% of those have a classe or activity recognized as "A". We'll now see how this method performs on the model testing subset. We'll mainly look at the confusion matrix and accuracy.

```{r modelTstDT}
predDTfit = predict(modDTfit, modTSTSample) #Get predictions on model testing subset

#Evaluate Decision Tree Performance on Validation sample
#Calculate Confusion Matrix, Accuracy and Error
confusionMatrix(modTSTSample$classe, predDTfit)
accuracy_DT <- postResample(predDTfit, modTSTSample$classe)
accuracy_DT
DTfit_SampleError = 1 - as.numeric(confusionMatrix(modTSTSample$classe, predDTfit)$overall[1])
DTfit_SampleError
```

The above shows a 49% accuracy which is poor and definitely has scope of improvement. Hence we'll try to improve upon it by using a random forest prediction mechanism.

#####__Method 2: Random Forest__

The _Random Forest_ mechanism extends the boosted aggregation technique on the classification/regression trees. This method, although is a bit more complex than the decision tree approach, it is one of the most accurate prediction mechanisms used. It takes care of correlations of predictors and outliers and hence is more robust. To assess different features of the data, we shall use a __5-fold cross validation__.

```{r buildRF}
modRFfit <- train(classe~.,method="rf",data=modTRNSample,trControl=trainControl(method="cv",n=5),tree=250)
modRFfit

```

We can represent the random forest variable importance in the following list and diagram in _Figure 3_:

```{r varImp_Plot, echo=FALSE}
varImp(modRFfit)
plot(modRFfit,log="y")
```

So, as usual, now we'll test this algorithm on the model testing dataset and see how accurate a prediction mechanism it is.

```{r testRF}
predRFfit <- predict(modRFfit, modTSTSample)
#Evaluate Random Forest Performance on Validation sample
#Calculate Confusion Matrix, Accuracy and Error
confusionMatrix(modTSTSample$classe, predRFfit)
accuracy_RF <- postResample(predRFfit, modTSTSample$classe)
accuracy_RF
RFfit_SampleError = 1 - as.numeric(confusionMatrix(modTSTSample$classe, predRFfit)$overall[1])
RFfit_SampleError
```

The random forest algorithm _**modRFfit**_ shows a great accuracy of > 99%. The confusion matrix shows an excellent fit and shows very less misclassified values. We hence, have reasons to believe that it is a good algorithm and can take it forward to test on the validation data (_tstDataNew_). The data to be used for validation has been cleaned up already before.
```{r validateRF}
#Prediction on RF on Test Data (tstDataNew) for model validation
PRED_RF_TST_DATA <- predict(modRFfit,newdata=tstDataNew)

PRED_RF_TST_DATA
```

The above results give 100% correct results in the Final Prediction Assignment Test. So, this Random Forest algorithm - _**modRFfit**_ also passes the validation test and a great algorithm to use in this case.

###__Conclusion__
To conclude, the basic idea of any machine learning algorithm to be of use is a combination of accuracy and scalability. For the purposes of our project - the classification tree approach, though was very easy to interpret and implement, was slightly less accurate in performance. But the random forest, though a little more complex, was easily scalable and way more accurate. Hence, the random forest technique was chosen as the final prediction algorithm.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

